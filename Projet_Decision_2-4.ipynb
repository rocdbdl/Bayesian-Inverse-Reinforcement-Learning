{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiPCvJETfOrk"
      },
      "source": [
        "Roc & Ethan 2023/2024 Project Decison in Uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVo2omFGas75"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54e5ngJzfTKj"
      },
      "source": [
        "1. Code a python class to represent a partially defined MDP.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6mFUcF5eh8R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class PartiallyDefinedMDP:\n",
        "\n",
        "    def __init__(self, states, actions, initial_state, discount_factor,\n",
        "                 features, rewards ):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.initial_state = initial_state\n",
        "        self.discount_factor = discount_factor\n",
        "        self.features = features  # dictionnary where the key is the state and the value is a list of features {0,1} of size n the number of features\n",
        "        self.rewards = rewards # list of rewards of size n the number of features\n",
        "\n",
        "\n",
        "    def transition_function(self, s, action, s_prime):\n",
        "        letter, number = s[0], int(s[1])\n",
        "        letter_prime, number_prime = s_prime[0], int(s_prime[1])\n",
        "\n",
        "        # if self.features[s][0] == 1 : # Il s'agit d'un etat but , sa valeur est fixée\n",
        "        #     return 0\n",
        "\n",
        "        #Cas où le mouvement est sur plus que une case, ou en diagonale\n",
        "        if abs(ord(letter) - ord(letter_prime)) >1 :\n",
        "            return 0 # La distance est supérieure à 1\n",
        "        if abs(number - number_prime) > 1:\n",
        "            return 0 # La distance est supérieure à 1\n",
        "        if abs(ord(letter) - ord(letter_prime)) == 1 and abs(number - number_prime) == 1:\n",
        "            return 0 # Déplacement en diagonale interdit\n",
        "\n",
        "\n",
        "        # Cas ou il reste dans la même case\n",
        "        #On recupere le dernier element de states (pour avoir la taille de la grille) on construit states de telle sorte que le dernier element soit la dernière case du tableau ie celle en haut a droite de l'exemple par exemple\n",
        "        biggeststate = self.states[-1]\n",
        "        biggestletter, biggestnumber = biggeststate[0], int(biggeststate[1])\n",
        "\n",
        "        if s == s_prime:\n",
        "            # Il reste dans la même case uniquement si l'action est dans une direction dans laquelle la grille est limitée\n",
        "            # par exemple si on est en haut de la grille et qu'on veut aller en haut, on reste dans la même case\n",
        "            if action == 'U' and ord(letter) == ord(biggestletter):\n",
        "                return 0.8\n",
        "            if action == 'D' and ord(letter) == ord('A'):\n",
        "                return 0.8\n",
        "            if action == 'L' and number == 1:\n",
        "                return 0.8\n",
        "            if action == 'R' and number == biggestnumber:\n",
        "                return 0.8\n",
        "            if (action =='U' or action=='D') and (number == 1 or number == biggestnumber) :\n",
        "              return 0.1\n",
        "            if (action =='L' or action=='R') and (ord(letter) == ord('A') or ord(letter) == ord(biggestletter)) :\n",
        "              return 0.1\n",
        "\n",
        "        # Cas ou s_prime est en haut ou en bas de s, alors que l'action est Droite ou Gauche\n",
        "        if (action == 'R' or action == \"L\") and abs(ord(letter) - ord(letter_prime)) == 1  :\n",
        "            return 0.1 # La probabilité de se déplacer en haut ou en bas avec une action\n",
        "                       # d'aller à droite ou à gauche est de 0.1 dans le cas ou il slip vers le haut ou le bas\n",
        "\n",
        "\n",
        "        # Cas ou s_prime est à gauche ou à droite de s, alors que l'action est haut ou bas\n",
        "        if (action == 'U' or action == \"D\") and abs(number - number_prime) == 1  :\n",
        "            return 0.1 # La probabilité de se déplacer à gauche ou à droite avec une action\n",
        "                        # d'aller en haut ou en bas est de 0.1 dans le cas ou il slip vers la gauche ou la droite\n",
        "\n",
        "        #Cas où tout se passe bien\n",
        "        if action == 'U' and ord(letter) - ord(letter_prime) == -1 :\n",
        "            return 0.8 # La probabilité de se déplacer en haut avec une action d'aller en haut est de 0.8\n",
        "        if action == 'D' and ord(letter) - ord(letter_prime) == 1 :\n",
        "            return 0.8 # La probabilité de se déplacer en bas avec une action d'aller en bas est de 0.8\n",
        "        if action == 'R' and number - number_prime == -1 :\n",
        "            return 0.8 # La probabilité de se déplacer à droite avec une action d'aller à droite est de 0.8\n",
        "        if action == 'L' and number - number_prime == 1 :\n",
        "            return 0.8 # La probabilité de se déplacer à gauche avec une action d'aller à gauche est de 0.8\n",
        "\n",
        "\n",
        "        return 0 # Si aucune des conditions précédentes n'est vérifiée, alors la probabilité de se déplacer est de 0\n",
        "\n",
        "\n",
        "    def reward_function(self, state):\n",
        "        features = np.array(self.features[state])\n",
        "        rewards = np.array(self.rewards)\n",
        "        return np.dot(features, rewards)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PEgb8hikfZR"
      },
      "source": [
        "2. Instantiate the gridworld running example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZPFPif0ujkX"
      },
      "outputs": [],
      "source": [
        "# Définir les états\n",
        "states = []\n",
        "for i in range(1, 9):\n",
        "    for j in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']:\n",
        "        states.append(j+str(i))\n",
        "\n",
        "# Définir les actions\n",
        "actions = ['R', 'L', 'U', 'D']\n",
        "\n",
        "# Groupes de features\n",
        "treasures = ['A1']\n",
        "bombs = ['C3', 'F2', 'F8', 'D8']\n",
        "muds = ['B1', 'B2', 'B3', 'B4', 'C4', 'D3', 'D4', 'D5', 'D6']\n",
        "waters = ['G2', 'G3']\n",
        "mountains = ['C3']\n",
        "\n",
        "discount_factor = 0.9\n",
        "\n",
        "# Définir les features\n",
        "features = {}\n",
        "for state in states:\n",
        "    feature_list = [0, 0, 0, 0, 0]  # Initialiser à [0, 0, 0, 0, 0]\n",
        "    if state in treasures:\n",
        "        feature_list[0] = 1  # Trésor\n",
        "    if state in bombs:\n",
        "        feature_list[1] = 1  # Bombe\n",
        "    if state in muds:\n",
        "        feature_list[2] = 1  # Boue\n",
        "    if state in waters:\n",
        "        feature_list[3] = 1  # Eau\n",
        "    if state in mountains:\n",
        "        feature_list[4] = 1  # Montagne\n",
        "    features[state] = feature_list\n",
        "\n",
        "\n",
        "# {Treasure: 0.04, Bomb: -0.05, Mud: -0.01, Water: -0.015, Mountain: -0.03}\n",
        "rewards = [0.04,-0.05,-0.01,-0.015,-0.03]\n",
        "\n",
        "# État initial\n",
        "initial_state = 'D7'\n",
        "\n",
        "# Instancier le GridWorld MDP\n",
        "grid_world = PartiallyDefinedMDP(states, actions, initial_state, discount_factor, features, rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxaCvzC-mq9W"
      },
      "source": [
        "3. Provide a method to create a similar but randomly generated gridworld MDP, where the size of the grid and the number of cells with each feature can be parameterized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "class RandomGridWorld(PartiallyDefinedMDP):\n",
        "\n",
        "\n",
        "    def __init__(self, grid_size, num_treasures, num_bombs, num_muds, num_waters, num_mountains,rewards):\n",
        "\n",
        "        self.grid_size = grid_size\n",
        "        self.states = []\n",
        "        self.features = {\n",
        "            'Treasure': [],\n",
        "            'Bomb': [],\n",
        "            'Mud': [],\n",
        "            'Water': [],\n",
        "            'Mountain': []\n",
        "        }\n",
        "        self.rewards = rewards\n",
        "\n",
        "        # Générer les états\n",
        "        letters = string.ascii_uppercase[:grid_size]\n",
        "        for i in range(1, grid_size+1):\n",
        "            for letter in letters:\n",
        "                state = letter + str(i)\n",
        "                self.states.append(state)\n",
        "\n",
        "        # Assigner aléatoirement les features\n",
        "        self.assign_features(num_treasures, num_bombs, num_muds, num_waters, num_mountains)\n",
        "\n",
        "        # Définir les autres composants du MDP\n",
        "        actions = [ 'R','L', 'U', 'D']\n",
        "        initial_state = random.choice(self.states)\n",
        "        discount_factor = 0.9\n",
        "        super().__init__(self.states, actions, initial_state, discount_factor, self.features, self.rewards)\n",
        "\n",
        "    def assign_features(self, num_treasures, num_bombs, num_muds, num_waters, num_mountains):\n",
        "        states = self.states.copy()\n",
        "        # Assigner les trésors\n",
        "        self.features['Treasure'] = random.sample(states, num_treasures)\n",
        "        # Assigner les bombes\n",
        "        self.features['Bomb'] = random.sample(states, num_bombs)\n",
        "        # Assigner les boues\n",
        "        self.features['Mud'] = random.sample(states, num_muds)\n",
        "        # Assigner les eaux\n",
        "        self.features['Water'] = random.sample(states, num_waters)\n",
        "        # Assigner les montagnes\n",
        "        self.features['Mountain'] = random.sample(states, num_mountains)\n",
        "\n",
        "        # Calculer les vecteurs de caractéristiques pour chaque état\n",
        "        for state in self.states:\n",
        "            # Créer un vecteur de caractéristiques pour chaque état, int(state in self.features[feature]) renvoie 1 si l'état est dans la liste des états de la feature,\n",
        "            # 0 sinon\n",
        "            feature_vector = [int(state in self.features[feature]) for feature in ['Treasure', 'Bomb', 'Mud', 'Water', 'Mountain']]\n",
        "            self.features[state] = feature_vector\n",
        "\n",
        "    def copy(self):\n",
        "        return RandomGridWorld(self.grid_size, len(self.features['Treasure']), len(self.features['Bomb']), len(self.features['Mud']), len(self.features['Water']), len(self.features['Mountain']), self.rewards)\n"
      ],
      "metadata": {
        "id": "DiVNjlOjiqTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_a7rM3XlRzl"
      },
      "source": [
        "4. Code a policy iteration algorithm, to solve such an MDP if a reward function is provided.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4ryy_yevZqP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class PolicyIteration:\n",
        "    def __init__(self, mdp):\n",
        "        self.mdp = mdp\n",
        "        self.states = mdp.states\n",
        "        self.actions = mdp.actions\n",
        "        self.transition_function = mdp.transition_function\n",
        "        self.reward_function = mdp.reward_function\n",
        "        self.discount_factor = mdp.discount_factor\n",
        "        # Initialiser une politique arbitraire, tous commencent avec la même action à droite\n",
        "        self.policy = {s: 'R' for s in self.states}\n",
        "        self.rewards = {s: self.mdp.reward_function(s) for s in self.states}\n",
        "        self.values = [] # Correspond à la valeur de chaque état durant les itérations\n",
        "\n",
        "    def evaluate_policy(self, policy):\n",
        "        \"\"\" Évalue la valeur de chaque état pour la politique donnée \"\"\"\n",
        "        self.values = self.rewards.copy()  # Initialiser les valeurs à la reward de chaque état\n",
        "        while True:\n",
        "            values_changed = False\n",
        "            for s in self.states:\n",
        "                old_value = self.values[s]\n",
        "                action = policy[s]\n",
        "                new_value = self.get_quality_value(s,action) # On évalue la valeur de l'état s uniquement en fonction de la policy actuelle,\n",
        "                                                                        #on ne prend pas en compte les autres actions\n",
        "                if abs(old_value - new_value) > 1e-2:  # Seuil de convergence\n",
        "                    values_changed = True\n",
        "                self.values[s] = new_value\n",
        "            if not values_changed: # SI aucune valeur n'a changé, on a convergé, on sort de la boucle\n",
        "                break\n",
        "        return self.values\n",
        "\n",
        "    def extract_policy(self):\n",
        "        \"\"\"\n",
        "        Extrait une nouvelle politique en maximisant la fonction de qualité # Policy Improvement Step\n",
        "        π′(s) = argmaxa∈Aqπ(s, a)\n",
        "        Teste toutes les actions possibles et retourne la valeur de la meilleure action\n",
        "        -> retourne la politique optimale : π′(s) = argmaxa∈Aqπ(s, a) pour l'état s\n",
        "        \"\"\"\n",
        "        new_policy = {}\n",
        "        for s in self.states:\n",
        "            new_policy[s] = max(self.actions, key=lambda a: self.get_quality_value(s, a))\n",
        "\n",
        "        return new_policy\n",
        "\n",
        "    def get_quality_value(self, state, action):\n",
        "        value = self.reward_function(state) # R(s,a)\n",
        "        for next_state in self.states:\n",
        "            transition_prob = self.transition_function(state, action, next_state)\n",
        "            value += self.discount_factor * transition_prob * self.values[next_state]\n",
        "        return value\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        iteration = 0\n",
        "        while True:\n",
        "            #print(f\"Itération {iteration}:\\n\")\n",
        "            values = self.evaluate_policy(self.policy)\n",
        "            #print(\"Valeurs :\")\n",
        "            #for state, value in values.items():\n",
        "               # print(f\"État {state} : Valeur {value:.2f}\")\n",
        "\n",
        "            new_policy = self.extract_policy()\n",
        "            #print(\"\\nPolitique :\\n\")\n",
        "            #for state, action in new_policy.items():\n",
        "            #    print(f\"État {state} : Action {action}\")\n",
        "\n",
        "            if new_policy == self.policy:\n",
        "            #   print(f\"\\n--> Politique convergée avec {iteration} iterations.\\n\")\n",
        "                break\n",
        "\n",
        "            self.policy = new_policy\n",
        "            iteration += 1\n",
        "\n",
        "        return self.policy, values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test de la classe Policy Iteration"
      ],
      "metadata": {
        "id": "C_EQown8j0Jy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "611eGD1fC53D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1ef0a5-ed0b-4e89-db94-044d8f277bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quel Exemple lancer ? :\n",
            "\n",
            "1 : Exemple du TD\n",
            "2 : Exemple Aléatoire\n",
            "--> : 1\n",
            "\n",
            "Policy iteration pour exercice du TD :\n",
            "\n",
            "\n",
            "Politique optimale :\n",
            "État A1 : Action R\n",
            "État A2 : Action R\n",
            "État A3 : Action U\n",
            "État B1 : Action D\n",
            "État B3 : Action U\n",
            "État C1 : Action D\n",
            "État C2 : Action R\n",
            "État C3 : Action R\n",
            "\n",
            "Valeurs optimales :\n",
            "État A1 : Valeur 1.26\n",
            "État A2 : Valeur 1.40\n",
            "État A3 : Valeur 1.62\n",
            "État B1 : Valeur 1.13\n",
            "État B3 : Valeur 1.80\n",
            "État C1 : Valeur 1.02\n",
            "État C2 : Valeur 0.80\n",
            "État C3 : Valeur 2.00\n",
            "\n",
            "Grille avec valeurs et policy optimales :\n",
            "\n",
            "      1    2    3 \n",
            "C  1.02↓ 0.80→ TTTT→ \n",
            "B  1.13↓      1.80↑ \n",
            "A  1.26→ 1.40→ 1.62↑ \n"
          ]
        }
      ],
      "source": [
        "####################### EXAMPLE TD #######################\n",
        "\n",
        "\n",
        "### Simple surcharge de la classe PartiallyDefinedMDP pour définir une fonction de transition\n",
        "# spécifique à notre exemple avec les probabilité correcte pour l'état s2 (0.7 et 0.3)  ###\n",
        "\n",
        "class PartiallyDefinedMDP_FOR_EXAMPLE(PartiallyDefinedMDP):\n",
        "    def transition_function(self, s, action, s_prime):\n",
        "        letter, number = s[0], int(s[1])\n",
        "        letter_prime, number_prime = s_prime[0], int(s_prime[1])\n",
        "\n",
        "        if self.features[s][0] == 1 : # Il s'agit d'un etat but , sa valeur est fixée\n",
        "            return 0\n",
        "        #Cas où le mouvement est sur plus que une case, ou en diagonale\n",
        "        if abs(ord(letter) - ord(letter_prime)) >1 :\n",
        "            return 0 # La distance est supérieure à 1\n",
        "        if abs(number - number_prime) > 1:\n",
        "            return 0 # La distance est supérieure à 1\n",
        "        if abs(ord(letter) - ord(letter_prime)) == 1 and abs(number - number_prime) == 1:\n",
        "            return 0 # Déplacement en diagonale interdit\n",
        "        # Cas ou il reste dans la même case\n",
        "        #On recupere le dernier element de states (pour avoir la taille de la grille) on construit states de telle sorte que le dernier element soit la dernière case du tableau ie celle en haut a droite de l'exemple par exemple\n",
        "        biggeststate = self.states[-1]\n",
        "        biggestletter, biggestnumber = biggeststate[0], int(biggeststate[1])\n",
        "        if s == s_prime:\n",
        "            if s == \"A2\": # unique\n",
        "                return 0.3\n",
        "            # Il reste dans la même case uniquement si l'action est dans une direction dans laquelle la grille est limitée\n",
        "            # par exemple si on est en haut de la grille et qu'on veut aller en haut, on reste dans la même case\n",
        "            if action == 'U' and ord(letter) == ord(biggestletter):\n",
        "                if s == \"A2\":\n",
        "                    return 0.7\n",
        "                return 1\n",
        "            if action == 'D' and ord(letter) == ord('A'):\n",
        "                if s == \"A2\":\n",
        "                    return 0.7\n",
        "                return 1\n",
        "            if action == 'L' and number == 1:\n",
        "                if s == \"A2\":\n",
        "                    return 0.7\n",
        "                return 1\n",
        "            if action == 'R' and number == biggestnumber:\n",
        "                if s == \"A2\":\n",
        "                    return 0.7\n",
        "                return 1\n",
        "            # Pour le reste des cas, il ne peut pas rester dans la même case... pas sur que ce soit utile, doit etre compris dans le return 0 de la fin\n",
        "            else :\n",
        "                return 0\n",
        "        # Cas ou s_prime est en haut ou en bas de s, alors que l'action est Droite ou Gauche\n",
        "        if (action == 'R' or action == \"L\") and abs(ord(letter) - ord(letter_prime)) == 1  :\n",
        "            return 0 # La probabilité de se déplacer en haut ou en bas avec une action\n",
        "                       # d'aller à droite ou à gauche est de 0.1 dans le cas ou il slip vers le haut ou le bas\n",
        "        # Cas ou s_prime est à gauche ou à droite de s, alors que l'action est haut ou bas\n",
        "        if (action == 'U' or action == \"D\") and abs(number - number_prime) == 1  :\n",
        "            return 0 # La probabilité de se déplacer à gauche ou à droite avec une action\n",
        "                        # d'aller en haut ou en bas est de 0.1 dans le cas ou il slip vers la gauche ou la droite\n",
        "        #Cas où tout se passe bien\n",
        "        if action == 'U' and ord(letter) - ord(letter_prime) == -1 :\n",
        "            if s == \"A2\":\n",
        "                return 0.7\n",
        "            return 1 # La probabilité de se déplacer en haut avec une action d'aller en haut est de 0.8\n",
        "        if action == 'D' and ord(letter) - ord(letter_prime) == 1 :\n",
        "            if s == \"A2\":\n",
        "                return 0.7\n",
        "            return 1 # La probabilité de se déplacer en bas avec une action d'aller en bas est de 0.8\n",
        "        if action == 'R' and number - number_prime == -1 :\n",
        "            if s == \"A2\":\n",
        "                return 0.7\n",
        "            return 1 # La probabilité de se déplacer à droite avec une action d'aller à droite est de 0.8\n",
        "        if action == 'L' and number - number_prime == 1 :\n",
        "            if s == \"A2\":\n",
        "                return 0.7\n",
        "            return 1 # La probabilité de se déplacer à gauche avec une action d'aller à gauche est de 0.8\n",
        "        return 0 # Si aucune des conditions précédentes n'est vérifiée, alors la probabilité de se déplacer est de 0\n",
        "\n",
        "def print_grid(states, optimal_policy, optimal_values, features):\n",
        "    grid_size = int(max(states, key=lambda s: int(s[1:]))[1:])\n",
        "    grid = [['' for _ in range(grid_size)] for _ in range(len(set([s[0] for s in states])))]\n",
        "    letters = [chr(ord('A') + i) for i in range(len(set([s[0] for s in states])))]\n",
        "    letters.reverse()  # Inverser l'ordre des lettres\n",
        "\n",
        "    # Calculer la largeur de l'espacement\n",
        "    if grid_size < 5:\n",
        "        spacing_width = 4\n",
        "    else:\n",
        "        spacing_width = 45 // grid_size\n",
        "\n",
        "    # Print column numbers\n",
        "    print(' ' * 3, end='')\n",
        "    for col in range(1, grid_size + 1):\n",
        "        print(f\"{col:{spacing_width}}\", end=' ')\n",
        "    print()\n",
        "\n",
        "    for row, letter in enumerate(letters):\n",
        "        # Print row letter\n",
        "        print(f\"{letter:2}\", end=' ')\n",
        "        for col in range(grid_size):\n",
        "            state = f\"{letter}{col + 1}\"\n",
        "            if state in states:\n",
        "                value = optimal_values[state]\n",
        "                action = optimal_policy[state]\n",
        "                treasure = features[state][0] == 1\n",
        "                if treasure:\n",
        "                    grid[row][col] = f\"TTTT\"\n",
        "                else:\n",
        "                    grid[row][col] = f\"{value:.2f}\"\n",
        "                if action == 'R':\n",
        "                    grid[row][col] += '→'\n",
        "                elif action == 'L':\n",
        "                    grid[row][col] += '←'\n",
        "                elif action == 'U':\n",
        "                    grid[row][col] += '↑'\n",
        "                elif action == 'D':\n",
        "                    grid[row][col] += '↓'\n",
        "                # if treasure:\n",
        "                #     grid[row][col] += 'T'\n",
        "            print(f\"{grid[row][col]:{spacing_width}}\", end=' ')\n",
        "        print()\n",
        "\n",
        "states = ['A1', 'A2', 'A3', 'B1','B3', 'C1', 'C2', 'C3']\n",
        "\n",
        "rewards = [2,-1 ] # 2 pour le trésor, -1 pour la bombe\n",
        "# Treasure correspond à l'état but de notre exercice\n",
        "actions = [ 'R','L', 'U', 'D']\n",
        "initial_state = 'A1'\n",
        "discount_factor = 0.9\n",
        "\n",
        "# Groupes de features\n",
        "treasures = ['C3']\n",
        "bombs = ['C2']\n",
        "features = {}\n",
        "for state in states:\n",
        "    feature_list = [0, 0]  # Initialiser à [0, 0, 0, 0, 0]\n",
        "    if state in treasures:\n",
        "        feature_list[0] = 1  # Trésor\n",
        "    if state in bombs:\n",
        "        feature_list[1] = 1  # Bombe\n",
        "\n",
        "    features[state] = feature_list\n",
        "\n",
        "mdp = PartiallyDefinedMDP_FOR_EXAMPLE(states, actions, initial_state, discount_factor, features, rewards)\n",
        "\n",
        "print(\"\\nQuel Exemple lancer ? :\\n\")\n",
        "print(\"1 : Exemple du TD\")\n",
        "print(\"2 : Exemple Aléatoire\")\n",
        "boucle = 1\n",
        "\n",
        "while boucle :\n",
        "    try :\n",
        "        choix = int(input(\"--> : \"))\n",
        "        boucle = False\n",
        "    except :\n",
        "        print(\"Veuillez entrer un chiffre\")\n",
        "        boucle = True\n",
        "\n",
        "if choix == 1:\n",
        "    # Créer une instance de PolicyIteration et résoudre le MDP\n",
        "    policy_iteration = PolicyIteration(mdp)\n",
        "    print(\"\\nPolicy iteration pour exercice du TD :\\n\")\n",
        "    optimal_policy, optimal_values = policy_iteration.policy_iteration()\n",
        "\n",
        "    # Afficher la politique optimale\n",
        "    print(\"\\nPolitique optimale :\")\n",
        "    for state, action in optimal_policy.items():\n",
        "        print(f\"État {state} : Action {action}\")\n",
        "\n",
        "    # Afficher les valeurs optimales\n",
        "    print(\"\\nValeurs optimales :\")\n",
        "    for state, value in optimal_values.items():\n",
        "        print(f\"État {state} : Valeur {value:.2f}\")\n",
        "\n",
        "    print(\"\\nGrille avec valeurs et policy optimales :\\n\")\n",
        "    print_grid(mdp.states, optimal_policy, optimal_values, mdp.features)\n",
        "\n",
        "\n",
        "\n",
        "# ############## EXAMPLE ALEATOIRE ####################\n",
        "\n",
        "if choix == 2 :\n",
        "    rewards = [ 2, -2, -0.5,-0.2,-1 ]\n",
        "\n",
        "    # Définir une instance de MDP\n",
        "    mdp = RandomGridWorld(9, 1, 1, 0, 0, 0,rewards)\n",
        "\n",
        "    # Créer une instance de PolicyIteration et résoudre le MDP\n",
        "    policy_iteration = PolicyIteration(mdp)\n",
        "    print(\"\\nPolicy iteration avec un exemple Aleatoire :\\n\")\n",
        "    optimal_policy, optimal_values = policy_iteration.policy_iteration()\n",
        "\n",
        "    # Afficher la politique optimale\n",
        "    print(\"\\nPolitique optimale :\")\n",
        "    for state, action in optimal_policy.items():\n",
        "        print(f\"État {state} : Action {action}\")\n",
        "\n",
        "    # Afficher les valeurs optimales\n",
        "    print(\"\\nValeurs optimales :\")\n",
        "    for state, value in optimal_values.items():\n",
        "        print(f\"État {state} : Valeur {value:.2f}\")\n",
        "\n",
        "    print(\"\\nGrille avec valeurs et policy optimales :\\n\")\n",
        "    print_grid(mdp.states, optimal_policy, optimal_values, mdp.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zTmZa0n0pNC"
      },
      "source": [
        "5. Provide a method which, given a reward function R, a number M of timesteps, and an initial state s0, provides a set O of M state-action pairs starting from s0. To simulate an imperfect tutor, the optimal action will be chosen with probability 0.95, and a random action is chosen otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdeJCblg0o3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844d3e77-a2c1-45b0-bf95-cdfb33d1d4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A1', 'R')\n",
            "('A2', 'R')\n",
            "('A3', 'U')\n",
            "('B3', 'U')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_stateActions(mdp, M, s0):\n",
        "    \"\"\"\n",
        "    Le but de cette fonction est de générer des state-actions à partir d'un MDP en suivant la politique optimale.\n",
        "    Le mdp pars de l'état initial s0 et génère M state-actions en suivant la politique optimale.\n",
        "    Il se déplace de l'état courant à l'état suivant en suivant la politique optimale avec une probabilité de 0.95\n",
        "\n",
        "    Args:\n",
        "        reward_function (dict): Le dictionnaire des récompenses pour chaque caractéristique.\n",
        "        M (int): Le nombre de state-actions à générer.\n",
        "        s0 (str): L'état initial à partir duquel commencer les déplacements.\n",
        "\n",
        "    Returns:\n",
        "        list: Une liste de tuples (state, action) représentant les state-actions générées.\n",
        "    \"\"\"\n",
        "    # Résoudre le MDP pour obtenir la politique optimale\n",
        "    policy_iteration = PolicyIteration(mdp)\n",
        "    optimal_policy, _ = policy_iteration.policy_iteration()\n",
        "\n",
        "    # Initialiser l'état courant\n",
        "    current_state = s0\n",
        "\n",
        "    # Liste pour stocker les observations\n",
        "    stateActions_pairs = []\n",
        "\n",
        "    # Générer les stateActions_pairs\n",
        "    for _ in range(M):\n",
        "\n",
        "        # Avec une probabilité de 0.95, choisir l'action optimale\n",
        "        # Sinon, choisir une action aléatoire\n",
        "        if random.random() < 0.95:\n",
        "            action = optimal_policy[current_state]\n",
        "        else:\n",
        "            action = random.choice(mdp.actions)\n",
        "\n",
        "        # Ajouter la state_action à la liste\n",
        "        stateActions_pairs.append((current_state, action))\n",
        "\n",
        "        # Effectuer la transition vers le prochain état\n",
        "        # Choisit un état au hasard de la liste mdp.states en utilisant les poids associés via la fonction de transition\n",
        "        # le [0] à la fin car random.choices retourne une liste\n",
        "        try :\n",
        "            next_state = random.choices(mdp.states, weights=[mdp.transition_function(current_state, action, s_prime) for s_prime in mdp.states])[0]\n",
        "        except :\n",
        "            continue\n",
        "        current_state = next_state\n",
        "\n",
        "        # En gros, on récupère l'action optimale pour l'état courant à l'aide de la policy, puis, comme on a une part d'aléatoire\n",
        "        # pour le déplacement, on choisit un état suivant en fonction de la probabilité de transition associée à chaque état en fonction\n",
        "        # de l'action optimale choisie.\n",
        "\n",
        "    return stateActions_pairs\n",
        "\n",
        "\n",
        "# Générer 10 observations à partir de l'état initial 'A1'\n",
        "stateActions = generate_stateActions(mdp, 10, 'A1')\n",
        "\n",
        "for s_a in stateActions:\n",
        "    print(s_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTL7ec_7Slv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9f7de0-019c-49fd-998f-370f856f79e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('B3', 'U')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'D')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n",
            "('C3', 'R')\n"
          ]
        }
      ],
      "source": [
        "#AFFICHAGE DES M STATES-ACTIONS calculées notamment grâce au mdp initialisé question 4\n",
        "\n",
        "stateActions = generate_stateActions(mdp, 15, 'B3')\n",
        "\n",
        "for s_a in stateActions:\n",
        "    print(s_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKf10HfZcN1"
      },
      "source": [
        "6. Provide a method to compute the ratio P(R1|O)/P(R2|O) given R1, R2, O, and a choice for the\n",
        "prior function. This is a meta step which will require to provide several classes and methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXfRY5o7dwry"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "from math import factorial\n",
        "\n",
        "\n",
        "class Prior(ABC):\n",
        "    \"\"\"\n",
        "    Classe abstraite pour représenter une distribution prior\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def compute_prior(self, R):\n",
        "        \"\"\"\n",
        "        Calcule la probabilité a priori P(R) pour la fonction de récompense R.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class UniformPrior(Prior):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_prior(self, R):\n",
        "        R_max = np.max(np.abs(R))\n",
        "        # Vérifier si toutes les récompenses sont dans l'intervalle [-R_max, R_max]\n",
        "        if np.all(np.abs(R) <= R_max):\n",
        "            return 1.0 / (2 * R_max) ** len(R)\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "class GaussianPrior(Prior):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_prior(self, R):\n",
        "        sigma = np.std(R) # Ecart type\n",
        "        probs = [np.exp(-(r ** 2) / (2 * sigma ** 2)) / (sigma * np.sqrt(2 * np.pi)) for r in R] # On calcule la probabilité pour chaque feature r\n",
        "        return np.prod(probs) # on fait le produit des probabilités pour chaque feature r\n",
        "\n",
        "class BetaPrior(Prior):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_prior(self, R):\n",
        "        R_max = max(np.abs(R))\n",
        "        alpha = len(R) + 1\n",
        "        beta = len(R) + 1\n",
        "        probs = []\n",
        "        for r in R:\n",
        "            if -R_max <= r <= R_max:\n",
        "                x = (r + R_max) / (2 * R_max)  # Normaliser r dans [0, 1]\n",
        "                prob = (gamma(alpha + beta) / (gamma(alpha) * gamma(beta))) * (x**(alpha - 1)) * ((1 - x)**(beta - 1))\n",
        "                probs.append(prob)\n",
        "            else:\n",
        "                probs.append(0)  # Si r est hors de [-R_max, R_max], la probabilité est 0\n",
        "        return np.prod(probs)\n",
        "\n",
        "\n",
        "class MixturePrior(Prior):\n",
        "    def __init__(self, priors, weights):\n",
        "        self.priors = priors\n",
        "        self.weights = weights\n",
        "\n",
        "    def compute_prior(self, R):\n",
        "        probs = [prior.compute_prior(R) * weight for prior, weight in zip(self.priors, self.weights)]\n",
        "        return sum(probs)\n",
        "\n",
        "class BayesianFramework:\n",
        "\n",
        "    def __init__(self, mdp, prior, alpha):\n",
        "        self.mdp = mdp\n",
        "        self.prior = prior\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def compute_likelihood(self, O, R):\n",
        "        \"\"\"\n",
        "        Calcule la vraisemblance P(O|R) étant donné les observations O et la fonction de récompense R.\n",
        "        \"\"\"\n",
        "        likelihood = 1.0\n",
        "        # On remplace les rewards par R\n",
        "        self.mdp.rewards = R\n",
        "        policy_iteration = PolicyIteration(self.mdp)\n",
        "        policy_iteration.policy_iteration() # On effectue la policy iteration pour obtenir les valeurs optimales de chaque état pour R\n",
        "        values = policy_iteration.values\n",
        "        # print(\"\\nLe grid world avec les valeurs optimales pour R :\",R, \" est :\\n\")\n",
        "        # print_grid(self.mdp.states, policy_iteration.policy, values, self.mdp.features)\n",
        "\n",
        "        for s, a in O: # Uniquement les etats issus des observations\n",
        "            value = self.mdp.reward_function(s) # Reward de l'état s\n",
        "            # calcule la liste des quality pour chaque action possible pour l'état s\n",
        "            q_values = [self.get_quality_value_R(s,a_prime,value,values) for a_prime in self.mdp.actions]\n",
        "            #print(\"La quality values pour l'état \",s,\" est :\\n\\t \",q_values)\n",
        "             # quality de l'action observée / somme des quality de toutes les actions possibles\n",
        "            action_index = self.mdp.actions.index(a)\n",
        "            #print(\"La quality values pour l'état \",s,\" et l'action \",a,\"avec r :\",R,  \", est :\\n\\t \",q_values[action_index])\n",
        "            prob_optimal_action = np.exp(self.alpha * q_values[action_index]) / sum(np.exp(self.alpha * q_val) for q_val in q_values)\n",
        "            likelihood *= prob_optimal_action\n",
        "\n",
        "        return likelihood\n",
        "\n",
        "    def compute_ratio(self, R1, R2, O):\n",
        "        \"\"\"\n",
        "        Calcule le ratio P(R1|O) / P(R2|O) étant donné R1, R2 et les observations O.\n",
        "        \"\"\"\n",
        "        likelihood_R1 = self.compute_likelihood(O, R1)\n",
        "        likelihood_R2 = self.compute_likelihood(O, R2)\n",
        "        prior_R1 = self.prior.compute_prior(R1)\n",
        "        prior_R2 = self.prior.compute_prior(R2)\n",
        "\n",
        "        ratio = (likelihood_R1 * prior_R1) / (likelihood_R2 * prior_R2)\n",
        "        return ratio\n",
        "\n",
        "    def get_quality_value_R(self, state, action,value,values):\n",
        "        #print(\"get quality , State : \",state, \"Action : \",action)\n",
        "\n",
        "        for next_state in self.mdp.states:\n",
        "            transition_prob = self.mdp.transition_function(state, action, next_state)\n",
        "            #print(\"Transition prob : \",transition_prob,\" vers \",next_state)\n",
        "            value += self.mdp.discount_factor * transition_prob * values[next_state]\n",
        "            #print(\"Value : \",value)\n",
        "        return value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test de la classe BayesianFramework\n"
      ],
      "metadata": {
        "id": "1R4BNHAMkPu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXSpQgDq4lJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32022768-cf0c-41e7-f28d-73549b51c2f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      1    2    3    4 \n",
            "D  10.85↓ 9.67← 8.61← 7.64← \n",
            "C  12.51↓ 10.99← 9.64← 8.45← \n",
            "B  TTTT← 12.34← 10.61← 9.15← \n",
            "A  10.27↑ 10.66↑ 9.48← 8.40← \n",
            "Ratio de Likelihood Uniform Prior, P(R1|O) / P(R2|O) : 1.136818489991992\n",
            "Ratio de Likelihood Gaussian Prior, P(R1|O) / P(R2|O) : 3.817498482338822\n",
            "Ratio de Likelihood Mixture Prior, P(R1|O) / P(R2|O) : 1.5448894508357254\n"
          ]
        }
      ],
      "source": [
        "rewards = [2,-2, -0.5,-0.2,-1]\n",
        "R1 = [3,-1, -0.75,-0.3,-1.5]\n",
        "R2 = [2,-2, -0.5,-0.2,-1]\n",
        "\n",
        "\n",
        "mdp = RandomGridWorld(4, 1, 1, 0, 0, 0, rewards)\n",
        "policy_iteration = PolicyIteration(mdp)\n",
        "optimal_policy, optimal_values = policy_iteration.policy_iteration()\n",
        "print_grid(mdp.states, optimal_policy, optimal_values, mdp.features)\n",
        "\n",
        "# Créer des instances de distributions a priori\n",
        "uniform_prior = UniformPrior()\n",
        "gaussian_prior = GaussianPrior()\n",
        "beta_prior = BetaPrior()\n",
        "mixture_prior = MixturePrior([uniform_prior, gaussian_prior], [0.5, 0.5])\n",
        "\n",
        "# Créer une instance de BayesianFramework\n",
        "alpha = 1.0  # Paramètre de confiance dans le tuteur\n",
        "\n",
        "# Générer 10 observations à partir de l'état initial 'A1'\n",
        "observations = generate_stateActions(mdp, 10, 'A1')\n",
        "\n",
        "# for s_a in stateActions:\n",
        "#     print(s_a)\n",
        "\n",
        "# Calculer le ratio de vraisemblance pour les fonctions de récompense R1 et R2\n",
        "bayesian_framework = BayesianFramework(mdp, uniform_prior, alpha)\n",
        "ratio = bayesian_framework.compute_ratio(R1, R2, observations)\n",
        "print(f\"Ratio de Likelihood Uniform Prior, P(R1|O) / P(R2|O) : {ratio}\")\n",
        "\n",
        "# Répéter avec d'autres distributions a priori\n",
        "bayesian_framework = BayesianFramework(mdp, gaussian_prior, alpha)\n",
        "ratio = bayesian_framework.compute_ratio(R1, R2, observations)\n",
        "print(f\"Ratio de Likelihood Gaussian Prior, P(R1|O) / P(R2|O) : {ratio}\")\n",
        "\n",
        "# bayesian_framework = BayesianFramework(mdp, beta_prior, alpha)\n",
        "# ratio = bayesian_framework.compute_ratio(R1, R2, observations)\n",
        "# print(f\"Ratio de Likelihood Beta Prior, P(R1|O) / P(R2|O) : {ratio}\")\n",
        "\n",
        "bayesian_framework = BayesianFramework(mdp, mixture_prior, alpha)\n",
        "ratio = bayesian_framework.compute_ratio(R1, R2, observations)\n",
        "print(f\"Ratio de Likelihood Mixture Prior, P(R1|O) / P(R2|O) : {ratio}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8VvqRzVBHdR"
      },
      "source": [
        "7. Code the PolicyWalk algorithm to return a reward with high a posteriori probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtXy8cKbBHBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a41e13-41a3-405a-98a3-35531d1a704e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleure fonction de récompense R : [1.0, 0.5, 0.5, 2.0, -0.5]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_neighbors(R, delta, R_max):\n",
        "    neighbors = []\n",
        "    for i in range(len(R)):\n",
        "        if -R_max <= R[i] + delta <= R_max:\n",
        "            neighbor = R.copy()\n",
        "            neighbor[i] += delta\n",
        "            neighbors.append(neighbor)\n",
        "        if -R_max <= R[i] - delta <= R_max:\n",
        "            neighbor = R.copy()\n",
        "            neighbor[i] -= delta\n",
        "            neighbors.append(neighbor)\n",
        "    return neighbors\n",
        "\n",
        "def generate_reward_space(R_max, delta):\n",
        "    \"\"\"Génére l'espace des récompenses en utilisant une grille de taille delta.\"\"\"\n",
        "    R_space = []\n",
        "    for i in range(int(2 * R_max /delta) + 1):\n",
        "        R_space.append(-R_max + i * delta)\n",
        "    return R_space\n",
        "\n",
        "def MajDictio(tuple, dictioPosterioriProbas) :\n",
        "    if tuple in dictioPosterioriProbas :\n",
        "      dictioPosterioriProbas[tuple] += 1\n",
        "    else :\n",
        "      dictioPosterioriProbas[tuple] = 1\n",
        "\n",
        "def quality_function(mdp, state, action, rewards, values):\n",
        "    reward_vector = np.array(rewards)\n",
        "    feature_vector = np.array(mdp.features[state])\n",
        "    value = np.dot(feature_vector, reward_vector)\n",
        "    for next_state in mdp.states:\n",
        "        transition_prob = mdp.transition_function(state, action, next_state)\n",
        "        value += mdp.discount_factor * transition_prob * values[next_state]\n",
        "    return value\n",
        "\n",
        "def policy_walk(prior, mdp, R_max, delta, num_iterations, alpha, observations):\n",
        "    #Création d'un dictio où chaque clé est un tuple correspondant aux rewardlists et chaque valeur, le nombre de fois où ces rewards apparaissent\n",
        "    dictioPosterioriProbas = {}\n",
        "    R_space = generate_reward_space(R_max, delta) # On construit l'espace des rewards\n",
        "    R = []\n",
        "    for i in range(len(mdp.rewards)):\n",
        "        R.append(random.choice(R_space)) # On initialise R avec des valeurs aléatoires dans l'espace des rewards\n",
        "    MajDictio(tuple(R), dictioPosterioriProbas)\n",
        "    mdpCopy = mdp.copy()\n",
        "    mdpCopy.rewards = R\n",
        "    policy_iteration = PolicyIteration(mdpCopy)\n",
        "    optimal_policy, optimal_values = policy_iteration.policy_iteration()\n",
        "    R_map = R.copy()  # Initialiser R_map avec les récompenses initiales\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        neighbors = get_neighbors(R, delta, R_max)\n",
        "        Rprime = neighbors[np.random.randint(len(neighbors))]\n",
        "\n",
        "        accept = False\n",
        "        for s in mdpCopy.states :\n",
        "            val = quality_function(mdp, s, optimal_policy[s], Rprime, optimal_values)\n",
        "            for a in mdpCopy.actions :\n",
        "                if quality_function(mdp, s, a, Rprime, optimal_values) > val :\n",
        "                    accept = True\n",
        "                    break\n",
        "            if accept == True :\n",
        "                break\n",
        "\n",
        "        BF = BayesianFramework(mdpCopy,prior, alpha)\n",
        "        if accept == True :\n",
        "            policy_iteration_bis = PolicyIteration(mdpCopy)\n",
        "            policy_iteration_bis.policy = optimal_policy  #car on commence de la politique optimal_policy, on ne repart pas de 0\n",
        "            optimal_policy_bis, optimal_values_bis = policy_iteration_bis.policy_iteration()\n",
        "\n",
        "\n",
        "            if random.random() < min(1,BF.compute_ratio(Rprime, R, observations)) :\n",
        "                R = Rprime\n",
        "                optimal_policy, policyiter_values = optimal_policy_bis, optimal_values_bis\n",
        "\n",
        "        elif random.random() < min(1,BF.compute_ratio(Rprime, R, observations)) :\n",
        "            R = Rprime\n",
        "\n",
        "        MajDictio(tuple(R), dictioPosterioriProbas)\n",
        "\n",
        "    Rmap = list(max(dictioPosterioriProbas, key=dictioPosterioriProbas.get))\n",
        "\n",
        "    return Rmap\n",
        "\n",
        "# Test de la fonction PolicyWalk\n",
        "# (Vous devez définir les classes manquantes comme RandomGridWorld, PolicyIteration, BayesianFramework, etc.)\n",
        "rewards = [2, -2, -0.5, -0.5, -1]\n",
        "mdp = RandomGridWorld(5, 1, 1, 0, 0, 0, rewards)\n",
        "prior = UniformPrior()\n",
        "stepsize = 0.5\n",
        "nb_iterations = 10\n",
        "R_max = max(np.abs(rewards))\n",
        "alpha_bayesian_frame = 1.0\n",
        "observation = generate_stateActions(mdp, 10, 'A1')\n",
        "R_best = policy_walk(prior, mdp, R_max, stepsize, nb_iterations, alpha_bayesian_frame, observation)\n",
        "print(f\"Meilleure fonction de récompense R : {R_best}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn5oxs0q0RbP"
      },
      "source": [
        "8. Provide such a modified version of your PolicyWalk algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZJzhl6fYI_u"
      },
      "source": [
        "9. Perform experiments on gridworld-like MDPs of varying characteristics. You will obtain a variety of plots giving the performance of your algorithms with different choices of parameters as a function of some other parameters (e.g., size of the MDP, number of features, number of iterations in the PolicyWalk algorithm, ...)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Définir les paramètres à faire varier\n",
        "grid_sizes = [3,4,5]\n",
        "num_features = [[1, 1, 1, 1, 1]]\n",
        "num_iterations = [3,5,10]\n",
        "priors = [UniformPrior()]\n",
        "alphas = [1]\n",
        "\n",
        "# Dictionnaire pour stocker les résultats\n",
        "results = {}\n",
        "\n",
        "for grid_size in grid_sizes:\n",
        "    for num_feat in num_features:\n",
        "        for n_iter in num_iterations:\n",
        "            for prior in priors:\n",
        "                for alpha in alphas:\n",
        "\n",
        "                    # Générer les gridworlds aléatoires et les observations\n",
        "                    policy_losses = []\n",
        "                    for _ in range(5):\n",
        "                        mdp = RandomGridWorld(grid_size,num_feat[0],num_feat[1],num_feat[2],num_feat[3],num_feat[4] ,[2, -2, -0.5, -0.2, -1])\n",
        "                        observations = generate_stateActions(mdp, 10, \"A1\")\n",
        "\n",
        "                        # Exécuter PolicyWalk\n",
        "                        R_map = policy_walk(prior, mdp, 1, 0.1, n_iter, alpha, observations)\n",
        "\n",
        "                        # Évaluer la performance\n",
        "                        mdp.rewards = R_map\n",
        "                        policy_iteration = PolicyIteration(mdp)\n",
        "                        optimal_policy, _ = policy_iteration.policy_iteration()\n",
        "\n",
        "                        policy_loss = 0\n",
        "                        for s, a in observations:\n",
        "                            if a != optimal_policy[s]:\n",
        "                                policy_loss += 1\n",
        "\n",
        "                        policy_losses.append(policy_loss)\n",
        "\n",
        "                    # Calculer les statistiques\n",
        "                    mean_loss = np.mean(policy_losses)\n",
        "                    std_loss = np.std(policy_losses)\n",
        "\n",
        "                    # Stocker les résultats\n",
        "                    key = (grid_size, tuple(num_feat), n_iter, str(prior), alpha)\n",
        "                    results[key] = (mean_loss, std_loss)\n",
        "\n",
        "# Afficher les résultats\n",
        "for key, (mean_loss, std_loss) in results.items():\n",
        "    grid_size, num_feat, n_iter, prior_str, alpha = key\n",
        "    print(f\"Grid Size: {grid_size}, #Features: {num_feat}, #Iterations: {n_iter}, Prior: {prior_str}, Alpha: {alpha}\")\n",
        "    print(f\"Mean Policy Loss: {mean_loss:.3f}, Std Dev: {std_loss:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Regrouper les résultats par (grid_size, num_feat)\n",
        "grouped_results = {}\n",
        "for key, (mean_loss, std_loss) in results.items():\n",
        "    grid_size, num_feat, n_iter, prior_str, alpha = key\n",
        "    group_key = (grid_size, tuple(num_feat))\n",
        "    if group_key not in grouped_results:\n",
        "        grouped_results[group_key] = []\n",
        "    grouped_results[group_key].append((n_iter, mean_loss))\n",
        "\n",
        "# Tracer les courbes\n",
        "for group_key, group_data in grouped_results.items():\n",
        "    grid_size, num_feat = group_key\n",
        "    iterations = [x[0] for x in group_data]\n",
        "    losses = [x[1] for x in group_data]\n",
        "\n",
        "    plt.plot(iterations, losses, label=f\"Grid Size: {grid_size}, #Features: {num_feat}\")\n",
        "\n",
        "plt.title(\"Policy Loss Curves\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Policy Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Afficher le graphique\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GscT7cPtiO2U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}